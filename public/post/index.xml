<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Mostly Engineering</title>
    <link>https://maciejkula.github.io/post/</link>
    <description>Recent content in Posts on Mostly Engineering</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Jul 2018 17:38:00 +0100</lastBuildDate>
    
	<atom:link href="https://maciejkula.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Building an autodifferentiation library</title>
      <link>https://maciejkula.github.io/2018/07/18/building-an-autodifferentiation-library/</link>
      <pubDate>Wed, 18 Jul 2018 17:38:00 +0100</pubDate>
      
      <guid>https://maciejkula.github.io/2018/07/18/building-an-autodifferentiation-library/</guid>
      <description>This blog post originally appeared on Medium
Popular general-purpose auto-differentiation frameworks like PyTorch or TensorFlow are very capable, and, for the most part, there is little need for writing something more specialized.
Nevertheless, I have recently started writing my own autodiff package. This blog post describes what I’ve learned along the way. Think of this as a poor-man’s version of a Julia Evans blog post.
Note that there are many blog posts describing the mechanics of autodifferentiation much better than I could, so I skip the explanations here.</description>
    </item>
    
    <item>
      <title>My approximate nearest neighbour talk at Europython 2015</title>
      <link>https://maciejkula.github.io/2015/08/06/my-approximate-nearest-neighbour-talk-at-europython-2015/</link>
      <pubDate>Thu, 06 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://maciejkula.github.io/2015/08/06/my-approximate-nearest-neighbour-talk-at-europython-2015/</guid>
      <description>At this year&amp;rsquo;s Europython, I presented a talk on &amp;lsquo;Speeding up search with locality sensitive hashing&amp;rsquo;.
In the talk, I presented the intuition behind locality sensitive hashing and discussed several Python packages for performing approximate nearest neighbour searches (including my own, rpforest).
If you are interested in the details, please see my blog post on the Lyst developers blog. The recording of the talk and slides are embedded below.
Talk:</description>
    </item>
    
    <item>
      <title>Simple MinHash implementation in Python</title>
      <link>https://maciejkula.github.io/2015/06/01/simple-minhash-implementation-in-python/</link>
      <pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://maciejkula.github.io/2015/06/01/simple-minhash-implementation-in-python/</guid>
      <description>MinHash is a simple but effective algorithm for estimating set similarity using the Jaccard index. Both the Wikipedia entry and this blog post are good explanations of how it works.
MinHash is attractive because it allows us to decide how similar two sets are without having to enumerate all of their elements. If we want to know how many users that performed action $A$ also performed action $B$, we can compare the MinHashes of the two sets instead of keeping track of multiple sets of millions of user ids.</description>
    </item>
    
    <item>
      <title>Incremental construction of sparse matrices</title>
      <link>https://maciejkula.github.io/2015/02/22/incremental-construction-of-sparse-matrices/</link>
      <pubDate>Sun, 22 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://maciejkula.github.io/2015/02/22/incremental-construction-of-sparse-matrices/</guid>
      <description>Sparse matrices are an indispensable tool &amp;ndash; because only non-zero entries are stored, they store information efficiently and enable (some) fast linear algera operations.
In Python, sparse matrix support is provided by scipy in scipy.sparse. They come in a number of flavours. Crucially, there are those that use efficient storage and/or support fast linear algebra operations (csr_matrix, csc_matrix, and coo_matrix), and those that enable efficient incremental construction and/or random element access (lil_matrix, dok_matrix).</description>
    </item>
    
    <item>
      <title>Calling BLAS from Cython</title>
      <link>https://maciejkula.github.io/2015/01/01/calling-blas-from-cython/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://maciejkula.github.io/2015/01/01/calling-blas-from-cython/</guid>
      <description>It is often useful to be able to call BLAS routines directly from Cython. Doing so avoids calling the corresponding NumPy functions (which would incur a performance penalty of running interpreted code and type and shape checking) as well as re-implementing linear algebra operations in Cython (which will likely be both incorrect and slower).
Existing Cython BLAS wrappers Correspondingly, there are several ways of doing so.
 CythonGSL provides Cython wrappers for the GNU Scientific Library.</description>
    </item>
    
  </channel>
</rss>