#+hugo_base_dir: .
#+hugo_section: ./post

#+hugo_weight: auto
#+hugo_auto_set_lastmod: t
* Engineering                                                  :@engineering:
** TODO Building a recommender system in Rust, part 1
:PROPERTIES:
:EXPORT_FILE_NAME: recommender-in-rust-part-1
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :math true
:END:
:LOGBOOK:
CLOCK: [2018-07-21 Sat 13:19]--[2018-07-21 Sat 13:26] =>  0:07
CLOCK: [2018-07-21 Sat 09:45]--[2018-07-21 Sat 10:15] =>  0:30
CLOCK: [2018-07-20 Fri 22:28]--[2018-07-20 Fri 22:58] =>  0:30
:END:

In this post, we're going to build a recommender system in Rust.
*** Setting up a project
Rust projects follow a certain structure, and we can set up a new project using ~cargo~:
#+BEGIN_SRC bash
cargo new --bin goodbooks-recommender
#+END_SRC

*** Getting the data
:LOGBOOK:
CLOCK: [2018-07-21 Sat 13:29]--[2018-07-21 Sat 13:38] =>  0:09
:END:

#+NAME: cargo-toml-preamble
#+BEGIN_SRC text :exports none
[package]
name = "goodbooks-recommender"
version = "0.1.0"
authors = ["Maciej Kula"]
#+END_SRC

The first dependency we are going to use is ~reqwest~: a crate similar to Python's ~requests~ that will allow us to easily download the data we need. The second is ~failure~, a crate that makes dealing with errors easier.

#+NAME: cargo-toml-reqwest
#+BEGIN_SRC text :exports none
reqwest = "0.8.6"
failure = "0.1.1"
serde = "1.0.0"
serde_derive = "1.0.0"
serde_json = "1.0.0"
csv = "1.0.0"
sbr = "0.4.0"
rand = "0.5.4"
#+END_SRC
**** Downloading data
:LOGBOOK:
CLOCK: [2018-07-21 Sat 19:20]--[2018-07-21 Sat 19:24] =>  0:04
CLOCK: [2018-07-21 Sat 18:55]--[2018-07-21 Sat 19:16] =>  0:21
:END:

With its help, we can start defining our download function:
#+NAME: main-download
#+BEGIN_SRC rust :exports code

  // Need to import a couple of things from
  // the standard library
  use std::fs::File;
  use std::io::BufWriter;
  use std::path::Path;

  /// Download file from `url` and save it to `destination`.
  fn download(url: &str, destination: &Path)
              -> Result<(), failure::Error> {

      // Don't do anything if we already have the file.
      if destination.exists() {
          return Ok(())
      }

      // Otherwise, create a new file.

      // Because each of the following operations
      // can fail (returns a result type), we follow
      // them with the `?` operator. If the result
      // is an error, it will exit from the function
      // early, propagating the result upwards; if
      // the operation completed successfully, we get
      // the result instead.
      let file = File::create(destination)?;
      let mut writer = BufWriter::new(file);

      let mut response = reqwest::get(url)?;
      response.copy_to(&mut writer)?;

      Ok(())
  }
#+END_SRC

With this, we can write a short function that downloads both the ratings file and a file that contains metadata on the books from the dataset:

#+NAME: main-get-data
#+BEGIN_SRC rust
  /// Download ratings and metadata both.
  fn download_data(ratings_path: &Path, books_path: &Path) {
      let ratings_url = "https://github.com/zygmuntz/\
                         goodbooks-10k/raw/master/ratings.csv";
      let books_url = "https://github.com/zygmuntz/\
                       goodbooks-10k/raw/master/books.csv";

      download(&ratings_url,
               ratings_path).expect("Could not download ratings");
      download(&books_url,
               books_path).expect("Could not download metadata");
  }
#+END_SRC

The ratings file looks like this:
#+BEGIN_EXAMPLE
user_id,book_id
9,8
15,398
15,275
37,7173
34,380
#+END_EXAMPLE

**** Parsing
:LOGBOOK:
CLOCK: [2018-07-22 Sun 19:26]--[2018-07-22 Sun 19:52] =>  0:26
CLOCK: [2018-07-21 Sat 13:44]--[2018-07-21 Sat 14:59] =>  1:15
:END:
We have two options for parsing the resulting CSV files. One is to parse things manually; the other, to use Rust's amazing serialization/deserialization capabilities and the [[https://crates.io/crates/csv][~csv~ crate]].

The heart of Rust's serialization ecosystem lies in the [[https://serde.rs/][~serde~ crate]]. It provides traits that allow structs to be seamlessly serialized and deserialized across a variety for formats. We'll derive those on a ~WishlistEntry~ struct to be able to read it from the CSV file:
#+NAME: main-wishlist-entry
#+BEGIN_SRC rust
  // Importing this allows us to autoderive
  // the serialization traits.
  #[macro_use]
  extern crate serde_derive;

  // This is where we get the serde traits from.
  extern crate serde;

  // An implementation of the serde encoders/decoders
  // to and from json. We'll need this later!
  extern crate serde_json;

  #[derive(Debug, Serialize, Deserialize)]
  struct WishlistEntry {
      user_id: usize,
      book_id: usize,
  }
#+END_SRC

After importing the ~csv~ crate we're ready to write the deserialize function:
#+NAME: main-deserialize
#+BEGIN_SRC rust
  extern crate csv;

  /// Deserialize from file at `path` into a vector of
  /// `WishlistEntry`.
  fn deserialize_ratings(path: &Path)
                 -> Result<Vec<WishlistEntry>, failure::Error> {

      let mut reader = csv::Reader::from_path(path)?;

      // We specify the type of the deserialized entity
      // via a type annotation. Otherwise, the compiler has
      // no way of knowing what sort of thing we want to
      // deserialize!
      // We also do a further trick where instead of deserializing
      // into a vector of results, we deserialize into a result with
      // a vector.
      let entries: Vec<WishlistEntry> = reader.deserialize()
          .collect::<Result<Vec<_>, _>>()?;

      Ok(entries)
  }
#+END_SRC

We also want to deserialize the metadata. We're only really interested in the book id and title, as this is what will allow us to make and evaluate recommendations based on titles rather than book ids.

As before, we define a struct and a corresponding deserialize function. This time, we are going to return two mappings instead of a vector: the first mapping book ids to book titles, the second book titles to book ids.
#+NAME: main-deserialize-metadata
#+BEGIN_SRC rust
  #[derive(Debug, Deserialize, Serialize)]
  struct Book {
      book_id: usize,
      title: String
  }

  // We'll use the stdlib hashmap for the mapping.
  use std::collections::HashMap;

  /// Deserialize from file at `path` into the book
  /// mappings.
  fn deserialize_books(path: &Path)
     -> Result<(HashMap<usize, String>,
                HashMap<String, usize>), failure::Error> {

      let mut reader = csv::Reader::from_path(path)?;

      let entries: Vec<Book> = reader.deserialize::<Book>()
          .collect::<Result<Vec<_>, _>>()?;

      // We can simply iterate over the entries and collect
      // them into a different data structure. This is not
      // the most efficient solution but it will do for now.
      let id_to_title: HashMap<usize, String> = entries
          .iter()
          .map(|book| (book.book_id, book.title.clone()))
          .collect();
      let title_to_id: HashMap<String, usize> = entries
          .iter()
          .map(|book| (book.title.clone(), book.book_id))
          .collect();

      Ok((id_to_title, title_to_id))
  }
#+END_SRC
*** Fitting a model
:LOGBOOK:
CLOCK: [2018-07-24 Tue 09:10]
CLOCK: [2018-07-23 Mon 18:56]--[2018-07-23 Mon 19:09] =>  0:13
:END:
Now that we have read the data, we can start thinking about what models to fit, and how to fit them.

The [[https://github.com/maciejkula/sbr-rs][~sbr~]] package implements two recommender models:
- an LSTM-based model, and 
- an exponential moving average (EWMA) model.

The first is much more powerful: it implements a full LSTM model, taking a user's history of past interactions and trying to predict their next action.

The second is simpler computationally: the user representation at time \(t\), \(u_t\) , is simply an exponentially weighted average of \(i_t\), the ($d$-dimensional) embeddings of items the user interacted with at time \(t\):
\[
   u_t = (1 - \sigma(\alpha))u_{t-1} + \sigma(\alpha)i_t,
\]
where \(\sigma(\alpha)\) is the exponential averaging weight, rescaled to lie between 0 and via the sigmoid function \(\sigma\).

Despite its simplicity, the model seems to perform fairly well on the Movielens dataset, and we're going to use it for this example.

**** Setting up hyperparameters
The first thing we need to do is to write a function that will set up all the hyperparameters of the model:
#+NAME: main-hyperparameters
#+BEGIN_SRC rust :noweb yes
  extern crate sbr;

  use sbr::models::ewma::{Hyperparameters, ImplicitEWMAModel};
  use sbr::models::{Loss, Optimizer};

  fn build_model(num_items: usize) -> ImplicitEWMAModel {
      let hyperparameters = Hyperparameters::new(num_items, 128)
          .embedding_dim(32)
          .learning_rate(0.16)
          .l2_penalty(0.0004)
          .loss(Loss::WARP)
          .optimizer(Optimizer::Adagrad)
          .num_epochs(10)
          .num_threads(1);

      hyperparameters.build()
  }
#+END_SRC

**** Preparing data
The second is to convert the ~WishlistEntry~ objects into ~sbr~'s [[https://docs.rs/sbr/0.4.0/sbr/data/struct.Interactions.html][~Interaction~]] objects:
#+NAME: main-interaction-convert
#+BEGIN_SRC rust :noweb yes
  use sbr::data::{Interaction, Interactions};

  fn build_interactions(data: &[WishlistEntry]) -> Interactions {
      // If the collection is empty, `max` doesn't exist. This
      // is why we get an Option back, which we then unwrap.
      let num_users = data
          .iter()
          .map(|x| x.user_id)
          .max()
          .unwrap() + 1;
      let num_items = data
          .iter()
          .map(|x| x.book_id)
          .max()
          .unwrap() + 1;

      let mut interactions = Interactions::new(num_users,
                                               num_items);

      // There are no timestamps in the interaction data, but
      // we make use of the fact that they are sorted by time.
      for (idx, datum) in data.iter().enumerate() {
          interactions.push(
              Interaction::new(datum.user_id,
                               datum.book_id,
                               idx)
          );
      }

      interactions
  }
#+END_SRC

**** Fitting
:LOGBOOK:
CLOCK: [2018-07-23 Mon 21:22]--[2018-07-23 Mon 21:43] =>  0:21
CLOCK: [2018-07-23 Mon 19:16]--[2018-07-23 Mon 19:20] =>  0:04
:END:
The model fitting itself is easy: we've set up the data and hyperparameters, and all that is left is to fit the model, making sure we have a train-test split to evaluate performance:
#+NAME: main-fit
#+BEGIN_SRC rust
  // We need to import the rand crate.
  extern crate rand;
  use rand::SeedableRng;

  // We perform a split where the train and test
  // sets are disjoint on the user dimension: no
  // single user is in both.
  use sbr::data::user_based_split;
  use sbr::OnlineRankingModel;

  use sbr::evaluation::mrr_score;

  /// Fit the model.
  ///
  /// If successful, return the MRR on the test set.
  /// Otherwise, return an error.
  fn fit(model: &mut ImplicitEWMAModel,
         data: &Interactions)
         -> Result<f32, failure::Error> {

      // Use a fixed seed for repeatable results.
      let mut rng = rand::XorShiftRng::from_seed([42; 16]);

      let (train, test) = user_based_split(data,
                                           &mut rng,
                                           0.2);

      model.fit(&train.to_compressed())?;

      let mrr = mrr_score(model, &test.to_compressed())?;

      Ok(mrr)
  }

#+END_SRC

*** Putting it all together
Finally, we can write our ~main~ function:
#+NAME: main-main
#+BEGIN_SRC rust :noweb yes
  fn main() {

      let ratings_path = Path::new("ratings.csv");
      let books_path = Path::new("books.csv");

      download_data(ratings_path, books_path);

      let ratings = deserialize_ratings(ratings_path).unwrap();
      let (id_to_title,
           title_to_id) = deserialize_books(books_path).unwrap();

      println!("Deserialized {} ratings.", ratings.len());
      println!("Deserialized {} books.", id_to_title.len());

      let interactions = build_interactions(&ratings);
      let mut model = build_model(interactions.num_items());

      println!("Fitting...");
      let mrr = fit(&mut model, &interactions).expect("Unable to fit model.");
      println!("Fit model with MRR of {:.2}", mrr);
  }
#+END_SRC
*** The finished article
**** Cargo.toml
#+NAME: cargo-toml-dependencies
#+BEGIN_SRC text :noweb yes :exports code :tangle code/goodbooks-recommender/Cargo.toml
<<cargo-toml-preamble>>

[dependencies]
<<cargo-toml-reqwest>>
#+END_SRC

**** main.rs
#+NAME: main
#+BEGIN_SRC rust :noweb yes :tangle code/goodbooks-recommender/src/main.rs
  extern crate reqwest;
  extern crate failure;

  <<main-wishlist-entry>>

  <<main-download>>
  <<main-deserialize>>
  <<main-deserialize-metadata>>
  <<main-hyperparameters>>
  <<main-interaction-convert>>
  <<main-fit>>

  <<main-get-data>>

  <<main-main>>
#+END_SRC



** DONE Building an autodifferentiation library                       :wyrm:
CLOSED: [2018-07-18 Wed 17:38]
:PROPERTIES:
:EXPORT_FILE_NAME: building-an-autodiff-library
:END:
/This blog post originally appeared on [[https://medium.com/@maciejkula/building-an-autodifferentiation-library-9ccf32c7a658][Medium]]/

Popular general-purpose [[https://en.wikipedia.org/wiki/Automatic_differentiation][auto-differentiation]] frameworks like PyTorch or TensorFlow are very capable, and, for the most part, there is little need for writing something more specialized.

Nevertheless, I have recently started writing my own autodiff package. This blog post describes what I’ve learned along the way. Think of this as a poor-man’s version of a [[https://jvns.ca/][Julia Evans]] blog post.

Note that there are many blog posts describing the mechanics of autodifferentiation much better than I could, so I skip the explanations here. Additionally, there are several other [[http://colah.github.io/posts/2015-09-NN-Types-FP/][interesting]] [[https://jeremyrsmith.github.io/scala-math-slides/#23][posts]] [[https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html][and]] [[https://arxiv.org/abs/1710.06892][articles]] on building type-safe neural networks constructs, so while my library follows very similar patterns (statically-typed graphs and dependent types), I don’t dwell on the type system angle too much.

Finally, In case you’d like to jump straight to the code, the end result is [[https://github.com/maciejkula/wyrm][here]], together with an obligatory neural-network based [[https://github.com/maciejkula/fizzbuzz][FizzBuzz solution]].
*** Motivation
There are a couple of reasons why I wanted to have my own autodiff/backprop framework, rather than use PyTorch or TensorFlow.

- PyTorch and TF are quite slow when fitting models that require little computation per minibatch. In computer vision problems so much computation is done per minibatch that framework overhead is mostly a non-issue. This isn’t true of fitting matrix-factorization-style models, useful in the recommender systems community. Even on a GPU, fitting these models is very slow.
- I want to be able to use my autodiff library to write and distribute models as Python packages with minimal dependencies. Being able to produce a fairly small and spelf-contained binary is an advantage over the rather heavy TF and PyTorch dependencies.
- It was a fun learning experience, and allowed me to understand the inner workings of mature neural network libraries in a little bit more detail.

Motivated by the desire for a lightweight solution that works well for recommender (and possibly NLP) models, I wrote down a list of design constraints.

- I want the framework to naturally support sparse gradients: cases where the vast majority of gradients are zero. This is very common in NLP and recommender models that use large embedding layers. In any given minibatch, only a very small proportion of the embedding layer is used, and the gradients of the remaining entries are zero. Being able to skip the zeros when performing a gradient update is essential in making these models fast.
- I want the framework to have minimal overhead on top of the actual computation. Since I mainly want to fit small, sparse models, overhead is key. In PyTorch, the run time of such models is dominated by the overhead of looping in Python. To avoid this, my library has to forego Python in its fitting loop, and be written entirely in a compiled language to take advantage of compiler optimizations.
- The models graphs have to be define-by-run, much like Chainer or PyTorch. The usability and debuggability of this approach is too valuable for me to even contemplate going back to the TensorFlow way of doing things. At the same time, I’m happy for the graph to be static once defined. This helps in keeping the overhead small: I can allocate intermediate computation buffers once and keep re-using them, instead of writing a complex buffer pool system (or, worse yet, repeatedly allocating and freeing memory on every pass).
- I want performance to scale approximately linearly with the number of available CPU cores. This means parallelizing at the level of the entire graph rather than individual operations. Each computation thread will have its own copy of the graph, but write to shared parameter buffers on update. This is effectively the Hogwild! approach, where multiple threads of computation update shared parameter buffers concurrently, without any locking. This allows near-linear scaling with little degradation in model quality as long as gradients are relatively sparse.

There is also a short list of things I don’t want, or don’t care enough about to add for now:

- GPU support. I mostly want to fit tiny models (or at least models with lots of parameters but little computation per minibatch).
- CNNs, or, indeed, tensors with more than two dimensions.

Given the list of requirements (and non-requirements), some design decisions follow naturally.

- The whole thing is going to be written in a compiled language that is capable of producing native shared objects with no runtime. Models will also be defined in the same language.
- That language is going to be [[https://www.rust-lang.org/][Rust]]. It’s an amazing language, and a perfect fit for this sort of task. For this reason, a lot of what follows has a Rust flavour. However, the design trade-offs I describe will (I believe) be the same in C++ and other statically typed and AOT compiled programming languages.
- I’m going to use [[https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation][reverse-mode autodifferentiation]]. That way, I can easily backpropagate through arbitrary (static) computation graphs with multiple inputs.

When writing libraries, I often think of the API I want to be able to expose and work back from there. In this case, I want to write something like the following:
#+BEGIN_SRC rust
   let slope = Parameter::new(1.0);
   let intercept = Parameter::new(0.0);
   let x = Input::new(3.0);
   let y = Input::new(2.0 * 3.0 + 1.0);
   let loss = (y — (slope * x + intercept)).square();
   loss.backward();
#+END_SRC

and have it just work.

Preliminaries done, we can move on to the fun part: figuring out how to implement the graph.
*** Representing the graph
What sort of data structure do we choose to represent the graph? I looked at two alternatives.

- Vector-based: all the computation nodes are stored contiguously in a vector, and use indices into that vector to address their parent nodes. For example, when creating an input node, an InputNode object is pushed onto the vector with index 0. If you then square that node, SquareNode is pushed onto the tape with index 1, knowing that its parent is an index 0. During a forward pass, the square node will use that index to get the value of its input.
- Graph-based. Nodes are placed at arbitrary locations in memory, and use references to their parents to maintain the graph structure. (The vector representation can be seen as a linearization of the graph-based model.)

#+BEGIN_SRC 
       Vector-based                              Graph-based

     +---------------+                       +-----------------+   
     |               |                       |                 |   
  +-->     A * B     <--+                +--->      A * B      <--+
  |  |               |  |                |   |                 |  |
  |  +---------------+  |                |   +-----------------+  |
  |  |               |  |                |                        |
  |  |       B       +--+                |                        |
  |  |               |                   |                        |
  |  +---------------+            +------+---------+    +---------+-------+
  |  |               |            |                |    |                 |
  +--+       A       |            |       A        |    |        B        |
     |               |            |                |    |                 |
     +---------------+            +----------------+    +-----------------+
#+END_SRC

There are a couple of advantages to the vector-based approach.
- All the nodes are in the same place. They are stored contiguously in memory, potentially reducing memory locality problems.
- It’s easy to reason about their ownership. This makes cloning the graph very easy: you just clone the node vector. This is important because I rely on having multiple copies of the graph for my parallelization approach.
- The nodes are arranged in topological order. We can correctly perform a forward pass with no duplicate work by simply iterating forward along the vector.

But there are also disadvantages.

It’s not clear what sort of object we are storing in the node vector. All of the nodes are different types (of different sizes), and vectors are homogeneously typed. Rust offers two solutions to this problem, but neither is fully satisfactory.

The first is [[https://doc.rust-lang.org/book/first-edition/enums.html][enums]] (sum types; ADTs; tagged unions). We define a ~Node~ type to be the union of all possible node types, and store that in the node vector. This way, everything has the same type. We still need to dispatch the node’s methods from the enclosing ~Node~ type to the contained inner node. This can be done via [[https://doc.rust-lang.org/book/first-edition/match.html][pattern matching]] (a switch statement on the tags of the union type); with Rust’s support for pattern matching and macros, writing the necessary code is a breeze.

However, this imposes a runtime cost. Every time we use a node, we need to go through the switch statement to resolve the inner type. In principle, optimizing compilers will compile such code to jump tables. In practice, the assembly generated for the dispatch code in my experiments was simply a linear scan over all the possibilities, imposing a dispatch cost that is linear in the number of concrete node types the framework supports. Worse still, the compiler is reluctant to inline both the switch itself and the called functions. The former is bad because it increases branch prediction misses, the latter increases function call overhead. (This problem is exacerbated by the recent branch-prediction attacks: it’s likely that [[http://archive.is/s831k][compiler mitigations]] will make indirect instructions like these substantially more expensive.)

The final disadvantage of using sum types for the node vector is that it results in a closed system (akin to Scala’s [[https://underscore.io/blog/posts/2015/06/02/everything-about-sealed.html][sealed traits]]): downstream users of the library cannot add new node types.

The alternative is to use Rust’s runtime polymorphism mechanism, [[https://doc.rust-lang.org/book/first-edition/trait-objects.html][trait objects]]. Trait objects are a way of abstracting over the concrete type of an object: instead of storing structs inline, we hide them behind a pointer to their data and a table of their methods. When calling a method, we jump to the vtable, find the function, and execute it. Using trait objects, we put these fat pointers into the node vector instead of nodes themselves.

This solution, however, introduces exactly the kind of indirection we set out to avoid in the first place. Additionally, it completely defeats the compiler’s efforts at inlinining: the function to be called is not known until runtime.

What about the graph-based design? Here, each node is placed in its own location in memory, and can refer to its ancestors via references. Because each node can be re-used an arbitrary number of times, I use Rust’s equivalent of a ~shared_ptr~ from C++, [[https://doc.rust-lang.org/std/rc/struct.Rc.html][~the Rc<T>~]].

One immediate disadvantage of this approach is that it blurs the ownership structure of the graph, making cloning and serialization/deserialization difficult: because nodes can be re-used, naive cloning/deserialization will result in multiple copies of the same nodes being created.

The second disadvantage is the lack of a readily-available topological ordering: both forward and backward passes have to be done recursively, and care has to be taken to avoid re-computing the values of shared subgraphs.

The advantage of using the graph representation is the types of any node’s parents are known at compile time. Every node is (recursively) generic over the types of its parents: adding two InputNodes will produce an ~AddNode<InputNode, InputNode>~. Adding that to another input node will produce an ~AddNode<AddNode<InputNode, InputNode>, InputNode>~ and so on. This gives me static method dispatch and the potential for inlining, in addition to a design that plays much more nicely with the type system.

*** Results
Using some informal benchmarks, the graph-based approach is approximately 30% faster than the vector-based approach. The end result can run a full epoch of a BPR learning-to-rank factorization model on the Movielens 100K dataset ([[https://github.com/maciejkula/wheedle/blob/master/src/lib.rs#L422%2529][code]]) in under 20 milliseconds on my puny dual-core laptop, and should scale linearly with more cores.

This takes advantage of a number of optimizations in addition to the underlying graph structure.

- I use Rust’s [[https://rust-lang-nursery.github.io/stdsimd/x86_64/stdsimd/][SIMD intrinsics]] for a number of operations, like vector dot products and scaled addition.
- For most operations, I assume C-contiguous matrices and iterate directly over the underlying data rather than use ~ndarrays~ [[https://docs.rs/ndarray/0.11.0/ndarray/iter/struct.Iter.html][iterator methods]]. This turns out to be much faster, presumably because it allows LLVM to autovectorize the loops.
- It turns out that LLVM is smart enough to autovectorize most numerical loops that don’t involve a reduction step (mostly assignments). Combined with (2), this makes a lot of numerical loops efficient with minimal optimization effort.

There are a number of ways to make the computation faster still.

1. At the moment, the code doesn’t do any subgraph result caching in the forward pass: if a node is used twice in the forward pass, all of the computations it depends on will be done twice. This can easily be solved via a simple topological sort algorithm, marking the nodes as evaluated once they have evaluated their value. (/Addendum: this turns out to be incredibly important for recurrent neural networks, so is now implemented./)
2. Similarly, gradients are passed straight to parameter nodes in the backward pass. If a node is used more than once, this means that unnecessary work is done in passing its gradients down one at a time. Accumulating all the gradients and only recursing once will save on that work. (/Addendum: as above./)
3. There is some unnecessary copying of inputs; making better use of references when possible should yield some small performance gains.

*** What’s next
I have written (and continue to maintain) a number of open-source Python ML packages. The models are written by hand in Cython, and while they perform well, extending them is tricky. This is due partly to Cython’s limitations, and partly due to the effort required for manual derivation of update rules.

I hope that this library (or some variation thereof) will make that task easier, and allow me to more easily implement complex models and release them as standalone Python packages. I’ll report back on how I fare.
*** Addendum

Turns out that the graph representation is a little bit problematic when applied to recurrent neural networks: at every step of the recurrence, the complexity of the resulting types increases, leading to rather baroque types:

#+BEGIN_SRC rust
Variable<nodes::LogNode<nodes::SoftmaxNode<nodes::DotNode<layers::recurrent::LSTMCellHidden<layers::recurrent::LSTMCellState<layers::recurrent::LSTMCellSt
ate<layers::recurrent::LSTMCellState<nodes::InputNode, nodes::InputNode, nodes::IndexNode<nodes::ParameterNode>>, layers::recurrent::LSTMCellHidden<nodes::InputNode, nodes::InputNode, nodes::IndexNode<nodes::Par
ameterNode>>, nodes::IndexNode<nodes::ParameterNode>>, layers::recurrent::LSTMCellHidden<layers::recurrent::LSTMCellState<nodes::InputNode, nodes::InputNode, nodes::IndexNode<nodes::ParameterNode>>, layers::recu
rrent::LSTMCellHidden<nodes::InputNode, nodes::InputNode, nodes::IndexNode<nodes::ParameterNode>>, nodes::IndexNode<nodes::ParameterNode>>, nodes::IndexNode<nodes::ParameterNode>>, layers::recurrent::LSTMCellHid
den<layers::recurrent::LSTMCellState<layers::recurrent::LSTMCellState<nodes::InputNode, nodes::InputNode, nodes::IndexNode<nodes::ParameterNode>>, layers::recurrent::LSTMCellHidden<nodes::InputNode, nodes::Input
Node, nodes::IndexNode<nodes::ParameterNode>>, nodes::IndexNode<nodes::ParameterNode>>, layers::recurrent::LSTMCellHidden<layers::recurrent::LSTMCellState<nodes::InputNode, nodes::InputNode, nodes::IndexNode<nod
es::ParameterNode>>, layers::recurrent::LSTMCellHidden<nodes::InputNode, nodes::InputNode, nodes::IndexNode<nodes::ParameterNode>>, nodes::IndexNode<nodes::ParameterNode>>, nodes::IndexNode<nodes::ParameterNode>
>, nodes::IndexNode<nodes::ParameterNode>>, nodes::ParameterNode>>>>
#+END_SRC

Needless to say, after a couple of recurrent steps the compiler gives up. This can be resolved by implementing a fused LSTM cell, rather than assembling it from simpler operations, or opting for selective type erasure via trait objects. So far, I’ve used the second solution: the output values of each LSTM cell have their concrete types erased by boxing them up in a trait object. Still, it illustrates the dangers of relying on complex type system constructs.
** DONE Don't use explicit feedback recommenders
CLOSED: [2018-07-19 Thu 19:02]
:PROPERTIES:
:EXPORT_FILE_NAME: dont-use-explicit
:END:
:LOGBOOK:
CLOCK: [2018-07-19 Thu 18:51]--[2018-07-19 Thu 19:02] =>  0:11
:END:
Back in January, I gave a talk at the [[https://www.meetup.com/RecSys-London/events/245357880/][London RecSys Meetup]] about why explicit feedback recommender models are inferior to implicit feedback models in the vast majority of cases.

The key argument is that what people choose to rate or not rate expresses a more fundamental preference than what the ratings is. Ignoring that preference and focusing on the gradations of preference /within/ ranked items is the wrong choice.

The slides are below, and you can watch the recording [[https://skillsmatter.com/skillscasts/11375-explicit-vs-implicit-recommenders][here]]. If you are interested in confirming this for yourself, have a look at my [[https://github.com/maciejkula/explicit-vs-implicit][explicit-vs-implicit experiment]].

#+BEGIN_EXPORT html
<script async class="speakerdeck-embed" data-id="c528f4ca53ec44969d34478b41806698" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>
#+END_EXPORT

** TODO Doubling down on emacs
:PROPERTIES:
:EXPORT_FILE_NAME: doubling-down-on-emacs
:END:
:LOGBOOK:
CLOCK: [2018-07-18 Wed 21:32]--[2018-07-18 Wed 21:43] =>  0:11
:END:

Over the last couple of weeks I've been revisiting my emacs config, paying particular attention to learning how to use ~org-mode~ effectively. I have in the past made several attempts at adopting it in my daily workflow, but have always found it too clunky to continue.

Needless to say, my previous experiences were entirely due to giving up too quickly, and not investing the time to find all the configuration options and packages that make it a great experience.

This post is mainly for my own benefit: I treat it as insurance against losing all the knowledge I've gleaned from various manuals and blog posts (especially [[https://zzamboni.org/post/my-emacs-configuration-with-commentary/][this one]]: it truly is a gem).

Disclaimer: I'm an emacs newbie, and I have /no idea/ how to write elisp. Be warned.

*** Org-mode settings

Firstly, a setting which should /really/ be a default:
#+BEGIN_SRC elisp
(setq org-startup-indented t)
#+END_SRC
This makes indentation work: without it, any text entered after an org-mode headline is not indented by default, making editing a real pain of manual indentation management. With it, everything is a breeze, just like indentation in any normal major mode for a programming language.

Secondly, allowing ~.gpg~ files to be picked up by the org-mode agenda:
#+BEGIN_SRC elisp
(unless (string-match-p "\\.gpg" org-agenda-file-regexp)
  (setq org-agenda-file-regexp
        (replace-regexp-in-string "\\\\\\.org" "\\\\.org\\\\(\\\\.gpg\\\\)?"
                                  org-agenda-file-regexp)))
#+END_SRC
This allows me to keep my agenda files encrypted, but still seamlessly decrypt them for constructing my agenda views.
*** Go settings
I've been using the Go programming language over the past year, and I've found the following make it look tolerable.

Firstly, reduce indentation width:
#+BEGIN_SRC elisp
(setq-default tab-width 4)
#+END_SRC

Secondly, lines in Go programs tend to be quite long: ~gofmt~ does not enforce a line length limit. The following settings wrap the lines and indent them pleasingly after wrapping:
#+BEGIN_SRC elisp
  ;; Ident wrapped lines: for Go codebases
  ;; that do not enforce a line length.
  (require 'adaptive-wrap)

  (with-eval-after-load 'adaptive-wrap
    (setq-default adaptive-wrap-extra-indent 2))

  ;; Only enable adaptive wrap in Go
  (add-hook 'go-mode-hook
    (lambda ()
      (adaptive-wrap-prefix-mode +1)))
#+END_SRC

For fun, you can also define an ~err-nil~ function, to save typing when dealing with Go's incredibly tedious error handling:
#+BEGIN_SRC elisp
  (defun err-nil ()
    "Insert if err != nil block"
    (interactive)
    (setq start (point))
    (insert "if err != nil {\nreturn nil, err\n}")
    (indent-region start (point))
    (previous-line)
    (indent-according-to-mode)
    )
#+END_SRC
(Needless to say, this doesn't work very well.)

** TODO Evolving LightFM                                           :lightfm:
:PROPERTIES:
:EXPORT_FILE_NAME: evolving-lightfm
:END:
:LOGBOOK:
CLOCK: [2018-07-19 Thu 12:36]--[2018-07-19 Thu 13:09] =>  0:33
CLOCK: [2018-07-19 Thu 09:00]--[2018-07-19 Thu 09:39] =>  0:39
:END:
[[https://github.com/lyst/lightfm][LightFM]] was first released in 2015, and has over time become one of the most popular packages for building recommender systems. It's [[https://stackshare.io/stream/stream-and-go-news-feeds-for-over-300-million-end-users][used]] [[https://medium.com/product-at-catalant-technologies/using-lightfm-to-recommend-projects-to-consultants-44084df7321c][widely]] [[https://www.inovex.de/fileadmin/files/Vortraege/2017/PyData-Recommender-florian-wilhelm-07.2017.pdf][in]] [[https://www.lyst.com][production]] and in [[https://scholar.google.co.uk/scholar?hl=en&as_sdt=0%252C5&q=lightfm+recommender+system&btnG=][research]].

My original intention for the package was to focus exclusively on the [[https://arxiv.org/abs/1507.08439][LightFM model]] rather than to attempt to build a wider framework incorporating multiple different models, united by common data formats and evaluation routines.

This has proven to be a reasonable approach. With some slight additions and bugfixes over the last three years, I now consider LightFM to be more or less a /finished product/ within the constraints of the original design.

However, I have come to believe that there are crucial features that LightFM lacks, and that cannot be addressed within the bounds of the single-model design. This blog posts sets out to outline the reasons why backwards incompatible evolution to LightFM v2 is necessary.

*** Fold-in
LightFM's chief problem is the lack of fold-in. Fold-in is an approach where new user representation can be estimated (or representations for existing users updated with new interactions) without model retraining.

I've come to view fold-in as something that a serious recommender system cannot do without. It has two chief uses:

1. Real-time updating of user representations. With fold-in, it's possible to update user representations (and what recommendations they are given) in real time as they interact with your product: any new interaction can be instantly affect the system's predictions. This stands in stark contrast with a system without fold-in, where user models are only updated after, at best, daily model retraining. This makes the system both less effective (cannot quickly adapt to changing preferences) and more costly to run (it depends more of frequent costly retraining for its effectiveness).
2. Training at scale. Without fold-in, factorization models need to be trained on every single user. If a user is not included in the training data, their representation will not be computed and they cannot be given recommendations. While LightFM is fast and parallelizes well, it is still likely that very large production system will find it impossible to scale it to their data. The solution here is sampled training. With fold-in, it's perfectly possible to sample a subset of users for model training, then fold-in the remaining users as needed.

Naturally, the lack of fold-in is not a problem unique to LightFM. To the best of my knowledge, there are no Python packages that implement it (please correct me!). Arguably, LightFM can deal with this better than many other libraries, as it is always possible to fold-in new users via their metadata features. Nevertheless, it remains a problem.

*** Challenges of adding fold-in
The obvious way to address the problem is to add (at least) user fold-in to the LightFM model; the implementation would run roughly along the following lines:

1. Obtain a user's interactions.
2. Initialize a random embedding vector for the user.
3. Take a number of SGD steps to update the embedding according to the data and the model's hyperparameters.
4. Return the resulting embedding for prediction.

I attempted to implement this, but I wasn't happy with the end result. The reasons fall roughly in two categories: firstly, the implementation is quite complex, and it stretches the existing Cython implementation to the breaking point. Secondly, other classes of models offer a much more natural way of handling the fold-in problem: I'd rather use those than try to shoehorn an ill-fitting solution onto the existing model.

**** Problem 1: Cython
I love Cython. It's a great enabler for Python programmers, and I am certain I could not have started writing high-performance Python packages without it.

However, I've found that that its usefulness is greatest for relatively simple programs, and diminishes as program complexity grows. As it tails off, the advantages of using another programming language (along the lines of C++) grow: as some point, it's helpful to be able to easily reach out for more fully fledged data structures like vectors and maps, and get further away from using the C programming model of pointers and arrays. While this is possible in Cython, I think doing so is harder than using C++ directly.

I think that further extensions to the LightFM code would push it past this threshold. Adding fold-in is certainly one such change.

**** Problem 2: there are more suitable models
While it is possible to add fold-in to classic factorization models, there are classes of model that handle the problem much more naturally, simply by virtue of how they approach user representations.

One such class of models is [[https://github.com/hidasib/GRU4Rec][sequence-based]] [[https://maciejkula.github.io/spotlight/index.html#sequential-models][models]]. Sequence-based models take the sequence of user actions as input and transform it into a representation useful for ranking candidates for recommendation. Here, adding new interactions is simply a matter of extracting predictions based on the new data: no model fitting is involved.

*** Way forward
Consequently, I lean towards (1) adding new models, and (2) implementing them in a language other than Cython, with greater access to external libraries, and better prospects for being extensible.

To that end, I have been working on a new recommender system library, [[https://github.com/maciejkula/sbr-rs][sbr]]. It's written in [[https://www.rust-lang.org/en-US/][Rust]], a new C++-like programming language, and implements (so far) two sequence models: an [[https://docs.rs/sbr/0.4.0/sbr/models/lstm/index.html][LSTM-based one]], and one based on a simple [[https://docs.rs/sbr/0.4.0/sbr/models/ewma/index.html][exponentially weighted average]] of a user's past interactions. Importantly, both are based on [[https://github.com/maciejkula/wyrm][wyrm]], a low-overhead autodifferentiation library. My hope is that this will allow new models to be constructed as easily as they can in libraries like PyTorch.

If you are a Rust user, you can try it out now. The general interface should be familiar to anyone currently using LightFM:
#+BEGIN_SRC rust
  extern crate sbr;
  extern crate rand;

  use std::time::Instant;
  use rand::SeedableRng;

  let mut data = sbr::datasets::download_movielens_100k().unwrap();

  let mut rng = rand::XorShiftRng::from_seed([42; 16]);

  let (train, test) = sbr::data::user_based_split(&mut data, &mut rng, 0.2);
  let train_mat = train.to_compressed();
  let test_mat = test.to_compressed();

  println!("Train: {}, test: {}", train.len(), test.len());

  let mut model = sbr::models::lstm::Hyperparameters::new(data.num_items(), 32)
      .embedding_dim(32)
      .learning_rate(0.16)
      .l2_penalty(0.0004)
      .lstm_variant(sbr::models::lstm::LSTMVariant::Normal)
      .loss(sbr::models::Loss::WARP)
      .optimizer(sbr::models::Optimizer::Adagrad)
      .num_epochs(10)
      .rng(rng)
      .build();

  let start = Instant::now();
  let loss = model.fit(&train_mat).unwrap();
  let elapsed = start.elapsed();
  let train_mrr = sbr::evaluation::mrr_score(&model, &train_mat).unwrap();
  let test_mrr = sbr::evaluation::mrr_score(&model, &test_mat).unwrap();
#+END_SRC
If you'd rather use it in Go, you can use the [[https://github.com/maciejkula/sbr-go][Go bindings]]. (You can also use it in other languages via its [[https://github.com/maciejkula/sbr-sys/blob/master/bindings.h][C bindings]].)

*** Evolving LightFM
I think ~sbr~ will prove to be a solid foundation for expanding and improving LightFM. However, adding it will be a radical departure from the original vision of LightFM as a package that does one thing, and one thing only: it will now be a framework.

Additionally, many of the assumptions valid for the current package will have to be revisited.

1. For sequence-based models (or adding time-varying intercepts), interaction timestamps will have to be present for all interactions. This means a departure from using simple ~scipy.sparse~ matrices as the main data structure for encoding training data.
2. For models capable of fold-in, train/test splitting and evaluation routines will have to change to allow testing on a validation set of users.

Taken together, these changes mean that the LightFM API will have to change substantially. There is no clean way of doing this in a backwards-compatible way, and so LightFM will evolve into a new major version, LightFM v2.

** TODO Thoughts on Go
:PROPERTIES:
:EXPORT_FILE_NAME: thoughts-on-go
:END:
Over the past year, I've had the opportunity to use the Go programming language in anger. This posts tries to summarize my overall impressions.

I was initially quite excited about trying Go. After using JVM languages, I was drawn by the promise of fast compile times and a lightweight runtime with first-class support of value types. 
* Footnotes
